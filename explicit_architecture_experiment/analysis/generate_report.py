#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_report.py
----------------------------------------------------
å®éªŒæŠ¥å‘Šç”Ÿæˆå™¨

åŠŸèƒ½ï¼š
1. æ•´åˆæ‰€æœ‰å®éªŒç»“æœ
2. ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
3. è¾“å‡ºå¯è§†åŒ–å›¾è¡¨
4. ç”Ÿæˆç»“è®ºå’Œå»ºè®®

ä¾èµ–ï¼š
    pip install pandas matplotlib seaborn scipy
----------------------------------------------------
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from typing import Dict, List, Any, Optional
from pathlib import Path
import argparse


class ExperimentReportGenerator:
    """å®éªŒæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.ensure_dir(output_dir)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
    def ensure_dir(self, path: str):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(path, exist_ok=True)
    
    def load_evaluation_results(self, explicit_eval_file: str, implicit_eval_file: str) -> Dict[str, Any]:
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        print("ğŸ“Š åŠ è½½è¯„ä¼°ç»“æœ...")
        
        # åŠ è½½æ˜¾æ€§æ¶æ„è¯„ä¼°ç»“æœ
        explicit_results = []
        with open(explicit_eval_file, 'r', encoding='utf-8') as f:
            for line in f:
                explicit_results.append(json.loads(line.strip()))
        
        # åŠ è½½éæ˜¾æ€§æ¶æ„è¯„ä¼°ç»“æœ
        implicit_results = []
        with open(implicit_eval_file, 'r', encoding='utf-8') as f:
            for line in f:
                implicit_results.append(json.loads(line.strip()))
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        explicit_metrics = self._compute_metrics(explicit_results)
        implicit_metrics = self._compute_metrics(implicit_results)
        
        # ç»Ÿè®¡æ£€éªŒ
        comparison_results = self._compare_metrics(explicit_metrics, implicit_metrics)
        
        return {
            'explicit_metrics': explicit_metrics,
            'implicit_metrics': implicit_metrics,
            'comparison': comparison_results
        }
    
    def load_coupling_analysis(self, coupling_report_file: str) -> Dict[str, Any]:
        """åŠ è½½è€¦åˆåº¦åˆ†æç»“æœ"""
        print("ğŸ” åŠ è½½è€¦åˆåº¦åˆ†æ...")
        
        # åŠ è½½CSVæŠ¥å‘Š
        df = pd.read_csv(coupling_report_file)
        
        # æŒ‰æ¶æ„ç±»å‹åˆ†ç»„
        explicit_df = df[df['view'] == 'explicit']
        implicit_df = df[df['view'] == 'non_explicit']
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        metrics = ['import_coupling', 'call_coupling', 'coupling_score']
        results = {}
        
        for metric in metrics:
            if metric in df.columns:
                exp_values = explicit_df[metric].values
                imp_values = implicit_df[metric].values
                
                # ç»Ÿè®¡æ£€éªŒ
                t_stat, t_p = stats.ttest_ind(exp_values, imp_values)
                u_stat, u_p = stats.mannwhitneyu(exp_values, imp_values, alternative='two-sided')
                
                results[metric] = {
                    'explicit_mean': np.mean(exp_values),
                    'explicit_std': np.std(exp_values),
                    'implicit_mean': np.mean(imp_values),
                    'implicit_std': np.std(imp_values),
                    'difference': np.mean(exp_values) - np.mean(imp_values),
                    't_statistic': t_stat,
                    't_p_value': t_p,
                    'u_statistic': u_stat,
                    'u_p_value': u_p
                }
        
        return results
    
    def load_attention_analysis(self, attention_analysis_dir: str) -> Dict[str, Any]:
        """åŠ è½½æ³¨æ„åŠ›åˆ†æç»“æœ"""
        print("ğŸ§  åŠ è½½æ³¨æ„åŠ›åˆ†æ...")
        
        # åŠ è½½æ³¨æ„åŠ›ç»Ÿè®¡ç»“æœ
        stats_file = os.path.join(attention_analysis_dir, "attention_statistics.json")
        if os.path.exists(stats_file):
            with open(stats_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {}
    
    def load_probe_analysis(self, probe_analysis_dir: str) -> Dict[str, Any]:
        """åŠ è½½æ¢é’ˆåˆ†æç»“æœ"""
        print("ğŸ§ª åŠ è½½æ¢é’ˆåˆ†æ...")
        
        # åŠ è½½æ¢é’ˆç»Ÿè®¡ç»“æœ
        stats_file = os.path.join(probe_analysis_dir, "probe_statistics.json")
        if os.path.exists(stats_file):
            with open(stats_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {}
    
    def _compute_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not results:
            return {}
        
        metrics = {}
        
        # åŸºæœ¬æŒ‡æ ‡
        if 'exact_match' in results[0]:
            exact_matches = [r['exact_match'] for r in results]
            metrics['exact_match_rate'] = np.mean(exact_matches)
            metrics['exact_match_std'] = np.std(exact_matches)
        
        if 'bleu' in results[0]:
            bleu_scores = [r['bleu'] for r in results]
            metrics['bleu_mean'] = np.mean(bleu_scores)
            metrics['bleu_std'] = np.std(bleu_scores)
        
        if 'rouge1' in results[0]:
            rouge1_scores = [r['rouge1'] for r in results]
            metrics['rouge1_mean'] = np.mean(rouge1_scores)
            metrics['rouge1_std'] = np.std(rouge1_scores)
        
        if 'rougeL' in results[0]:
            rougeL_scores = [r['rougeL'] for r in results]
            metrics['rougeL_mean'] = np.mean(rougeL_scores)
            metrics['rougeL_std'] = np.std(rougeL_scores)
        
        if 'meteor' in results[0]:
            meteor_scores = [r['meteor'] for r in results]
            metrics['meteor_mean'] = np.mean(meteor_scores)
            metrics['meteor_std'] = np.std(meteor_scores)
        
        return metrics
    
    def _compare_metrics(self, explicit_metrics: Dict[str, float], 
                        implicit_metrics: Dict[str, float]) -> Dict[str, Any]:
        """æ¯”è¾ƒæŒ‡æ ‡"""
        comparison = {}
        
        for metric in explicit_metrics.keys():
            if metric in implicit_metrics:
                exp_values = [explicit_metrics[metric]]
                imp_values = [implicit_metrics[metric]]
                
                # ç®€å•çš„æ¯”è¾ƒï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤šæ ·æœ¬ï¼‰
                comparison[metric] = {
                    'explicit_value': explicit_metrics[metric],
                    'implicit_value': implicit_metrics[metric],
                    'difference': explicit_metrics[metric] - implicit_metrics[metric],
                    'improvement': (explicit_metrics[metric] - implicit_metrics[metric]) / implicit_metrics[metric] * 100
                }
        
        return comparison
    
    def generate_comprehensive_report(self, evaluation_results: Dict[str, Any],
                                    coupling_results: Dict[str, Any],
                                    attention_results: Dict[str, Any],
                                    probe_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        
        report = {
            'experiment_summary': {
                'title': 'æ˜¾æ€§æ¶æ„å®éªŒç»¼åˆæŠ¥å‘Š',
                'date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_metrics_analyzed': 0,
                'significant_differences': 0,
                'overall_conclusion': ''
            },
            'performance_analysis': evaluation_results,
            'coupling_analysis': coupling_results,
            'attention_analysis': attention_results,
            'probe_analysis': probe_results,
            'conclusions_and_recommendations': {}
        }
        
        # ç»Ÿè®¡æ˜¾è‘—å·®å¼‚
        significant_count = 0
        total_metrics = 0
        
        # æ€§èƒ½æŒ‡æ ‡
        if 'comparison' in evaluation_results:
            for metric, comparison in evaluation_results['comparison'].items():
                total_metrics += 1
                if 't_p_value' in comparison and comparison['t_p_value'] < 0.05:
                    significant_count += 1
        
        # è€¦åˆåº¦æŒ‡æ ‡
        for metric, results in coupling_results.items():
            total_metrics += 1
            if results['t_p_value'] < 0.05:
                significant_count += 1
        
        # æ³¨æ„åŠ›æŒ‡æ ‡
        if attention_results:
            total_metrics += 2  # entropy å’Œ concentration
            if 'entropy_comparison' in attention_results and attention_results['entropy_comparison']['t_p_value'] < 0.05:
                significant_count += 1
            if 'concentration_comparison' in attention_results and attention_results['concentration_comparison']['t_p_value'] < 0.05:
                significant_count += 1
        
        # æ¢é’ˆæŒ‡æ ‡
        if probe_results:
            total_metrics += 1
            if 'accuracy_comparison' in probe_results and probe_results['accuracy_comparison']['t_p_value'] < 0.05:
                significant_count += 1
        
        report['experiment_summary']['total_metrics_analyzed'] = total_metrics
        report['experiment_summary']['significant_differences'] = significant_count
        
        # ç”Ÿæˆç»“è®º
        conclusions = self._generate_conclusions(report)
        report['conclusions_and_recommendations'] = conclusions
        
        return report
    
    def _generate_conclusions(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“è®ºå’Œå»ºè®®"""
        conclusions = {
            'key_findings': [],
            'statistical_significance': {},
            'practical_implications': [],
            'limitations': [],
            'future_work': []
        }
        
        # å…³é”®å‘ç°
        significant_count = report['experiment_summary']['significant_differences']
        total_metrics = report['experiment_summary']['total_metrics_analyzed']
        
        if significant_count > total_metrics * 0.5:
            conclusions['key_findings'].append("æ˜¾æ€§æ¶æ„åœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºéæ˜¾æ€§æ¶æ„")
        elif significant_count > 0:
            conclusions['key_findings'].append("æ˜¾æ€§æ¶æ„åœ¨éƒ¨åˆ†ç»´åº¦ä¸Šä¼˜äºéæ˜¾æ€§æ¶æ„")
        else:
            conclusions['key_findings'].append("æ˜¾æ€§æ¶æ„å’Œéæ˜¾æ€§æ¶æ„æ²¡æœ‰æ˜¾è‘—å·®å¼‚")
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§
        conclusions['statistical_significance'] = {
            'significant_metrics': significant_count,
            'total_metrics': total_metrics,
            'significance_rate': significant_count / total_metrics if total_metrics > 0 else 0
        }
        
        # å®é™…æ„ä¹‰
        if significant_count > 0:
            conclusions['practical_implications'].append("å»ºè®®åœ¨è½¯ä»¶å¼€å‘ä¸­é‡‡ç”¨æ˜¾æ€§æ¶æ„è®¾è®¡åŸåˆ™")
            conclusions['practical_implications'].append("æ˜¾æ€§æ¶æ„æœ‰åŠ©äºæå‡AIæ¨¡å‹åœ¨ä»£ç ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½")
        
        # å±€é™æ€§
        conclusions['limitations'].append("å®éªŒæ ·æœ¬æ•°é‡æœ‰é™")
        conclusions['limitations'].append("ä¸»è¦é’ˆå¯¹Pythonä»£ç ï¼Œå…¶ä»–è¯­è¨€éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
        conclusions['limitations'].append("æ¨¡å‹è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œå¤§è§„æ¨¡æ¨¡å‹çš„æ•ˆæœéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶")
        
        # æœªæ¥å·¥ä½œ
        conclusions['future_work'].append("æ‰©å±•åˆ°æ›´å¤šç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®ç±»å‹")
        conclusions['future_work'].append("ä½¿ç”¨æ›´å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹")
        conclusions['future_work'].append("ç ”ç©¶æ˜¾æ€§æ¶æ„åœ¨ä¸åŒä»£ç ä»»åŠ¡ä¸Šçš„æ•ˆæœ")
        conclusions['future_work'].append("å¼€å‘è‡ªåŠ¨åŒ–çš„æ˜¾æ€§æ¶æ„æ£€æµ‹å’Œä¼˜åŒ–å·¥å…·")
        
        return conclusions
    
    def create_visualizations(self, report: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        # 1. æ€§èƒ½å¯¹æ¯”å›¾
        self._create_performance_comparison_chart(report)
        
        # 2. è€¦åˆåº¦å¯¹æ¯”å›¾
        self._create_coupling_comparison_chart(report)
        
        # 3. ç»¼åˆç»“æœå›¾
        self._create_comprehensive_results_chart(report)
    
    def _create_performance_comparison_chart(self, report: Dict[str, Any]):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾"""
        if 'performance_analysis' not in report:
            return
        
        performance = report['performance_analysis']
        if 'comparison' not in performance:
            return
        
        # æå–æ•°æ®
        metrics = list(performance['comparison'].keys())
        explicit_values = [performance['comparison'][m]['explicit_value'] for m in metrics]
        implicit_values = [performance['comparison'][m]['implicit_value'] for m in metrics]
        
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, explicit_values, width, label='æ˜¾æ€§æ¶æ„', alpha=0.8)
        ax.bar(x + width/2, implicit_values, width, label='éæ˜¾æ€§æ¶æ„', alpha=0.8)
        
        ax.set_xlabel('æ€§èƒ½æŒ‡æ ‡')
        ax.set_ylabel('åˆ†æ•°')
        ax.set_title('æ˜¾æ€§æ¶æ„ vs éæ˜¾æ€§æ¶æ„ - æ€§èƒ½å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'performance_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def _create_coupling_comparison_chart(self, report: Dict[str, Any]):
        """åˆ›å»ºè€¦åˆåº¦å¯¹æ¯”å›¾"""
        if 'coupling_analysis' not in report:
            return
        
        coupling = report['coupling_analysis']
        
        # æå–æ•°æ®
        metrics = list(coupling.keys())
        explicit_means = [coupling[m]['explicit_mean'] for m in metrics]
        implicit_means = [coupling[m]['implicit_mean'] for m in metrics]
        explicit_stds = [coupling[m]['explicit_std'] for m in metrics]
        implicit_stds = [coupling[m]['implicit_std'] for m in metrics]
        
        # åˆ›å»ºå›¾è¡¨
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, explicit_means, width, yerr=explicit_stds, 
               label='æ˜¾æ€§æ¶æ„', alpha=0.8, capsize=5)
        ax.bar(x + width/2, implicit_means, width, yerr=implicit_stds, 
              label='éæ˜¾æ€§æ¶æ„', alpha=0.8, capsize=5)
        
        ax.set_xlabel('è€¦åˆåº¦æŒ‡æ ‡')
        ax.set_ylabel('è€¦åˆåº¦åˆ†æ•°')
        ax.set_title('æ˜¾æ€§æ¶æ„ vs éæ˜¾æ€§æ¶æ„ - è€¦åˆåº¦å¯¹æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels(metrics, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'coupling_comparison.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def _create_comprehensive_results_chart(self, report: Dict[str, Any]):
        """åˆ›å»ºç»¼åˆç»“æœå›¾"""
        # åˆ›å»ºæ€»ç»“å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ˜¾è‘—å·®å¼‚ç»Ÿè®¡
        significant_count = report['experiment_summary']['significant_differences']
        total_metrics = report['experiment_summary']['total_metrics_analyzed']
        
        axes[0, 0].pie([significant_count, total_metrics - significant_count], 
                      labels=['æ˜¾è‘—å·®å¼‚', 'æ— æ˜¾è‘—å·®å¼‚'], 
                      autopct='%1.1f%%', startangle=90)
        axes[0, 0].set_title('ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†å¸ƒ')
        
        # 2. æ€§èƒ½æå‡ç™¾åˆ†æ¯”
        if 'performance_analysis' in report and 'comparison' in report['performance_analysis']:
            improvements = []
            for metric, comparison in report['performance_analysis']['comparison'].items():
                if 'improvement' in comparison:
                    improvements.append(comparison['improvement'])
            
            if improvements:
                axes[0, 1].bar(range(len(improvements)), improvements, alpha=0.7)
                axes[0, 1].set_title('æ€§èƒ½æå‡ç™¾åˆ†æ¯”')
                axes[0, 1].set_ylabel('æå‡ç™¾åˆ†æ¯” (%)')
                axes[0, 1].grid(True, alpha=0.3)
        
        # 3. è€¦åˆåº¦å·®å¼‚
        if 'coupling_analysis' in report:
            differences = []
            for metric, results in report['coupling_analysis'].items():
                differences.append(results['difference'])
            
            axes[1, 0].bar(range(len(differences)), differences, alpha=0.7)
            axes[1, 0].set_title('è€¦åˆåº¦å·®å¼‚')
            axes[1, 0].set_ylabel('å·®å¼‚å€¼')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. å®éªŒæ€»ç»“
        axes[1, 1].text(0.1, 0.5, f"""
        å®éªŒæ€»ç»“:
        
        æ€»æŒ‡æ ‡æ•°: {total_metrics}
        æ˜¾è‘—å·®å¼‚: {significant_count}
        æ˜¾è‘—æ€§ç‡: {significant_count/total_metrics*100:.1f}%
        
        ç»“è®º: {'æ˜¾æ€§æ¶æ„æ˜¾è‘—ä¼˜äºéæ˜¾æ€§æ¶æ„' if significant_count > total_metrics*0.5 else 'éœ€è¦æ›´å¤šè¯æ®'}
        """, fontsize=12, verticalalignment='center',
        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
        axes[1, 1].set_title('å®éªŒæ€»ç»“')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'comprehensive_results.png'), 
                   dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_report(self, report: Dict[str, Any], output_file: str):
        """ä¿å­˜æŠ¥å‘Š"""
        print(f"ğŸ’¾ ä¿å­˜æŠ¥å‘Šåˆ°: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print("âœ… æŠ¥å‘Šä¿å­˜å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæ˜¾æ€§æ¶æ„å®éªŒæŠ¥å‘Š")
    parser.add_argument("--explicit_eval", type=str, required=True,
                       help="æ˜¾æ€§æ¶æ„è¯„ä¼°ç»“æœæ–‡ä»¶")
    parser.add_argument("--implicit_eval", type=str, required=True,
                       help="éæ˜¾æ€§æ¶æ„è¯„ä¼°ç»“æœæ–‡ä»¶")
    parser.add_argument("--coupling_report", type=str, required=True,
                       help="è€¦åˆåº¦æŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("--attention_analysis", type=str, required=True,
                       help="æ³¨æ„åŠ›åˆ†æç»“æœç›®å½•")
    parser.add_argument("--probe_analysis", type=str, required=True,
                       help="æ¢é’ˆåˆ†æç»“æœç›®å½•")
    parser.add_argument("--output", type=str, default="./final_report.json",
                       help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="./report_output",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    generator = ExperimentReportGenerator(args.output_dir)
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    evaluation_results = generator.load_evaluation_results(args.explicit_eval, args.implicit_eval)
    coupling_results = generator.load_coupling_analysis(args.coupling_report)
    attention_results = generator.load_attention_analysis(args.attention_analysis)
    probe_results = generator.load_probe_analysis(args.probe_analysis)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    comprehensive_report = generator.generate_comprehensive_report(
        evaluation_results, coupling_results, attention_results, probe_results
    )
    
    # åˆ›å»ºå¯è§†åŒ–
    generator.create_visualizations(comprehensive_report)
    
    # ä¿å­˜æŠ¥å‘Š
    generator.save_report(comprehensive_report, args.output)
    
    print("ğŸ¯ å®éªŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {args.output}")
    print(f"ğŸ“ˆ å›¾è¡¨ç›®å½•: {args.output_dir}")


if __name__ == "__main__":
    main()
