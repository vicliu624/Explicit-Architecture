#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
probe_trainer.py
----------------------------------------------------
çº¿æ€§æ¢é’ˆè®­ç»ƒå·¥å…·

åŠŸèƒ½ï¼š
1. æå–æ¨¡å‹ä¸­é—´å±‚è¡¨å¾
2. è®­ç»ƒçº¿æ€§æ¢é’ˆåˆ†ç±»å™¨
3. åˆ†æè¡¨å¾çš„å¯åˆ†æ€§
4. æ¯”è¾ƒæ˜¾æ€§å’Œéæ˜¾æ€§æ¶æ„çš„è¡¨å¾å·®å¼‚

ä¾èµ–ï¼š
    pip install transformers torch scikit-learn matplotlib seaborn
----------------------------------------------------
"""

import os
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from tqdm import tqdm
import argparse

from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from scipy.stats import ttest_ind, mannwhitneyu


class ProbeTrainer:
    """çº¿æ€§æ¢é’ˆè®­ç»ƒå™¨"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        self.scaler = StandardScaler()
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            use_fast=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¯ç”¨éšè—çŠ¶æ€è¾“å‡ºï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=self.device,
            output_hidden_states=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   å±‚æ•°: {self.model.config.n_layer}")
        print(f"   éšè—ç»´åº¦: {self.model.config.n_embd}")
    
    def extract_representations(self, text: str, layer_idx: int = -1, 
                              max_length: int = 512) -> np.ndarray:
        """æå–æŒ‡å®šå±‚çš„è¡¨å¾"""
        # Tokenize
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        ).to(self.model.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        
        # æå–æŒ‡å®šå±‚çš„éšè—çŠ¶æ€
        hidden_states = outputs.hidden_states[layer_idx]  # (batch, seq, hidden)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        representations = hidden_states[0].cpu().numpy()  # (seq, hidden)
        
        return representations
    
    def extract_token_representations(self, text: str, layer_idx: int = -1,
                                    max_length: int = 512) -> Tuple[np.ndarray, List[str]]:
        """æå–tokençº§åˆ«çš„è¡¨å¾"""
        # Tokenize
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        ).to(self.model.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        
        # æå–æŒ‡å®šå±‚çš„éšè—çŠ¶æ€
        hidden_states = outputs.hidden_states[layer_idx]  # (batch, seq, hidden)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        representations = hidden_states[0].cpu().numpy()  # (seq, hidden)
        
        # è·å–tokens
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        
        return representations, tokens
    
    def create_architecture_labels(self, data: List[Dict[str, Any]]) -> Tuple[np.ndarray, List[str]]:
        """åˆ›å»ºæ¶æ„æ ‡ç­¾"""
        X = []
        y = []
        texts = []
        
        for sample in data:
            text = sample.get('input', '')
            view = sample.get('view', '')
            
            if not text or not view:
                continue
            
            # æå–è¡¨å¾
            try:
                representations = self.extract_representations(text)
                # ä½¿ç”¨å¹³å‡æ± åŒ–
                avg_representation = np.mean(representations, axis=0)
                
                X.append(avg_representation)
                y.append(1 if view == 'explicit' else 0)
                texts.append(text)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                continue
        
        return np.array(X), np.array(y), texts
    
    def create_semantic_labels(self, data: List[Dict[str, Any]]) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """åˆ›å»ºè¯­ä¹‰æ ‡ç­¾ï¼ˆåŸºäºè€¦åˆåº¦ï¼‰"""
        X = []
        y_coupling = []
        y_architecture = []
        texts = []
        
        for sample in data:
            text = sample.get('input', '')
            view = sample.get('view', '')
            coupling = sample.get('coupling', {})
            
            if not text or not view:
                continue
            
            # æå–è¡¨å¾
            try:
                representations = self.extract_representations(text)
                avg_representation = np.mean(representations, axis=0)
                
                X.append(avg_representation)
                y_coupling.append(coupling.get('coupling_score', 0))
                y_architecture.append(1 if view == 'explicit' else 0)
                texts.append(text)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                continue
        
        return np.array(X), np.array(y_coupling), np.array(y_architecture), texts
    
    def train_architecture_probe(self, X: np.ndarray, y: np.ndarray, 
                              test_size: float = 0.2) -> Dict[str, Any]:
        """è®­ç»ƒæ¶æ„åˆ†ç±»æ¢é’ˆ"""
        print("ğŸ”§ è®­ç»ƒæ¶æ„åˆ†ç±»æ¢é’ˆ...")
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒå¤šç§åˆ†ç±»å™¨
        classifiers = {
            'logistic_regression': LogisticRegression(max_iter=1000, random_state=42),
            'svm': SVC(kernel='rbf', random_state=42),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42)
        }
        
        results = {}
        for name, clf in classifiers.items():
            print(f"   è®­ç»ƒ {name}...")
            
            # è®­ç»ƒ
            clf.fit(X_train_scaled, y_train)
            
            # é¢„æµ‹
            y_pred = clf.predict(X_test_scaled)
            y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1] if hasattr(clf, 'predict_proba') else None
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_pred)
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)
            
            results[name] = {
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'predictions': y_pred,
                'probabilities': y_pred_proba,
                'classifier': clf
            }
            
            print(f"     å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"     äº¤å‰éªŒè¯: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # é€‰æ‹©æœ€ä½³åˆ†ç±»å™¨
        best_name = max(results.keys(), key=lambda k: results[k]['accuracy'])
        best_results = results[best_name]
        
        print(f"âœ… æœ€ä½³åˆ†ç±»å™¨: {best_name} (å‡†ç¡®ç‡: {best_results['accuracy']:.4f})")
        
        return {
            'results': results,
            'best_classifier': best_name,
            'best_results': best_results,
            'X_test': X_test,
            'y_test': y_test,
            'X_test_scaled': X_test_scaled
        }
    
    def train_coupling_probe(self, X: np.ndarray, y_coupling: np.ndarray,
                           test_size: float = 0.2) -> Dict[str, Any]:
        """è®­ç»ƒè€¦åˆåº¦å›å½’æ¢é’ˆ"""
        print("ğŸ”§ è®­ç»ƒè€¦åˆåº¦å›å½’æ¢é’ˆ...")
        
        # åˆ’åˆ†è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_coupling, test_size=test_size, random_state=42
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒå›å½’å™¨
        regressor = LinearRegression()
        regressor.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_pred = regressor.predict(X_test_scaled)
        
        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import mean_squared_error, r2_score
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"   å‡æ–¹è¯¯å·®: {mse:.4f}")
        print(f"   RÂ²åˆ†æ•°: {r2:.4f}")
        
        return {
            'regressor': regressor,
            'mse': mse,
            'r2': r2,
            'predictions': y_pred,
            'y_test': y_test,
            'X_test_scaled': X_test_scaled
        }
    
    def analyze_layer_representations(self, data: List[Dict[str, Any]], 
                                   output_dir: str):
        """åˆ†æä¸åŒå±‚çš„è¡¨å¾è´¨é‡"""
        print("ğŸ” åˆ†æä¸åŒå±‚çš„è¡¨å¾è´¨é‡...")
        
        # ä¸ºæ¯ä¸ªå±‚è®­ç»ƒæ¢é’ˆ
        layer_results = {}
        
        for layer_idx in range(self.model.config.n_layer):
            print(f"   åˆ†æå±‚ {layer_idx}...")
            
            # æå–è¯¥å±‚çš„è¡¨å¾
            X = []
            y = []
            
            for sample in data:
                text = sample.get('input', '')
                view = sample.get('view', '')
                
                if not text or not view:
                    continue
                
                try:
                    representations = self.extract_representations(text, layer_idx)
                    avg_representation = np.mean(representations, axis=0)
                    
                    X.append(avg_representation)
                    y.append(1 if view == 'explicit' else 0)
                except Exception as e:
                    continue
            
            if len(X) < 10:  # æ ·æœ¬å¤ªå°‘
                continue
            
            X = np.array(X)
            y = np.array(y)
            
            # è®­ç»ƒæ¢é’ˆ
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # è®­ç»ƒåˆ†ç±»å™¨
            clf = LogisticRegression(max_iter=1000, random_state=42)
            clf.fit(X_train_scaled, y_train)
            
            # è¯„ä¼°
            accuracy = clf.score(X_test_scaled, y_test)
            cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)
            
            layer_results[layer_idx] = {
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
        
        # å¯è§†åŒ–ç»“æœ
        self._visualize_layer_analysis(layer_results, output_dir)
        
        return layer_results
    
    def _visualize_layer_analysis(self, layer_results: Dict[int, Dict[str, float]], 
                                output_dir: str):
        """å¯è§†åŒ–å±‚åˆ†æç»“æœ"""
        ensure_dir(output_dir)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # æå–æ•°æ®
        layers = list(layer_results.keys())
        accuracies = [layer_results[layer]['accuracy'] for layer in layers]
        cv_means = [layer_results[layer]['cv_mean'] for layer in layers]
        cv_stds = [layer_results[layer]['cv_std'] for layer in layers]
        
        # 1. å‡†ç¡®ç‡éšå±‚æ•°å˜åŒ–
        axes[0].plot(layers, accuracies, 'o-', color='blue', linewidth=2, markersize=6)
        axes[0].set_xlabel('å±‚æ•°')
        axes[0].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
        axes[0].set_title('å„å±‚è¡¨å¾åˆ†ç±»å‡†ç¡®ç‡')
        axes[0].grid(True, alpha=0.3)
        axes[0].set_ylim(0, 1)
        
        # 2. äº¤å‰éªŒè¯ç»“æœ
        axes[1].errorbar(layers, cv_means, yerr=cv_stds, fmt='o-', 
                        color='red', linewidth=2, markersize=6, capsize=5)
        axes[1].set_xlabel('å±‚æ•°')
        axes[1].set_ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
        axes[1].set_title('å„å±‚è¡¨å¾äº¤å‰éªŒè¯ç»“æœ')
        axes[1].grid(True, alpha=0.3)
        axes[1].set_ylim(0, 1)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        layer_analysis_path = os.path.join(output_dir, "layer_analysis.png")
        plt.savefig(layer_analysis_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å±‚åˆ†æå›¾è¡¨å·²ä¿å­˜: {layer_analysis_path}")
        plt.show()
    
    def compare_architectures(self, explicit_data: List[Dict[str, Any]],
                           implicit_data: List[Dict[str, Any]],
                           output_dir: str):
        """æ¯”è¾ƒæ˜¾æ€§å’Œéæ˜¾æ€§æ¶æ„çš„è¡¨å¾å·®å¼‚"""
        print("ğŸ” æ¯”è¾ƒæ¶æ„è¡¨å¾å·®å¼‚...")
        
        # æå–è¡¨å¾
        exp_X, exp_y, exp_texts = self.create_architecture_labels(explicit_data)
        imp_X, imp_y, imp_texts = self.create_architecture_labels(implicit_data)
        
        print(f"   æ˜¾æ€§æ¶æ„æ ·æœ¬: {len(exp_X)}")
        print(f"   éæ˜¾æ€§æ¶æ„æ ·æœ¬: {len(imp_X)}")
        
        # è®­ç»ƒæ¢é’ˆ
        exp_results = self.train_architecture_probe(exp_X, exp_y)
        imp_results = self.train_architecture_probe(imp_X, imp_y)
        
        # æ¯”è¾ƒç»“æœ
        self._create_comparison_plots(exp_results, imp_results, output_dir)
        
        # ç»Ÿè®¡æ£€éªŒ
        self._statistical_comparison(exp_results, imp_results, output_dir)
        
        print(f"ğŸ“Š æ¶æ„è¡¨å¾æ¯”è¾ƒå®Œæˆ: {output_dir}")
    
    def _create_comparison_plots(self, exp_results: Dict[str, Any],
                                imp_results: Dict[str, Any],
                                output_dir: str):
        """åˆ›å»ºæ¯”è¾ƒå›¾"""
        ensure_dir(output_dir)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. å‡†ç¡®ç‡å¯¹æ¯”
        classifiers = list(exp_results['results'].keys())
        exp_accuracies = [exp_results['results'][name]['accuracy'] for name in classifiers]
        imp_accuracies = [imp_results['results'][name]['accuracy'] for name in classifiers]
        
        x = np.arange(len(classifiers))
        width = 0.35
        
        axes[0, 0].bar(x - width/2, exp_accuracies, width, label='æ˜¾æ€§æ¶æ„', alpha=0.7)
        axes[0, 0].bar(x + width/2, imp_accuracies, width, label='éæ˜¾æ€§æ¶æ„', alpha=0.7)
        axes[0, 0].set_xlabel('åˆ†ç±»å™¨')
        axes[0, 0].set_ylabel('å‡†ç¡®ç‡')
        axes[0, 0].set_title('åˆ†ç±»å™¨å‡†ç¡®ç‡å¯¹æ¯”')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(classifiers, rotation=45)
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. äº¤å‰éªŒè¯å¯¹æ¯”
        exp_cv_means = [exp_results['results'][name]['cv_mean'] for name in classifiers]
        imp_cv_means = [imp_results['results'][name]['cv_mean'] for name in classifiers]
        exp_cv_stds = [exp_results['results'][name]['cv_std'] for name in classifiers]
        imp_cv_stds = [imp_results['results'][name]['cv_std'] for name in classifiers]
        
        axes[0, 1].errorbar(x, exp_cv_means, yerr=exp_cv_stds, fmt='o-', 
                           label='æ˜¾æ€§æ¶æ„', capsize=5)
        axes[0, 1].errorbar(x, imp_cv_means, yerr=imp_cv_stds, fmt='s-', 
                           label='éæ˜¾æ€§æ¶æ„', capsize=5)
        axes[0, 1].set_xlabel('åˆ†ç±»å™¨')
        axes[0, 1].set_ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
        axes[0, 1].set_title('äº¤å‰éªŒè¯ç»“æœå¯¹æ¯”')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(classifiers, rotation=45)
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ··æ·†çŸ©é˜µï¼ˆæœ€ä½³åˆ†ç±»å™¨ï¼‰
        exp_best = exp_results['best_results']
        imp_best = imp_results['best_results']
        
        exp_cm = confusion_matrix(exp_results['y_test'], exp_best['predictions'])
        imp_cm = confusion_matrix(imp_results['y_test'], imp_best['predictions'])
        
        sns.heatmap(exp_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
        axes[1, 0].set_title('æ˜¾æ€§æ¶æ„æ··æ·†çŸ©é˜µ')
        axes[1, 0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1, 0].set_ylabel('çœŸå®æ ‡ç­¾')
        
        sns.heatmap(imp_cm, annot=True, fmt='d', cmap='Reds', ax=axes[1, 1])
        axes[1, 1].set_title('éæ˜¾æ€§æ¶æ„æ··æ·†çŸ©é˜µ')
        axes[1, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[1, 1].set_ylabel('çœŸå®æ ‡ç­¾')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        comparison_path = os.path.join(output_dir, "probe_comparison.png")
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ¢é’ˆæ¯”è¾ƒå›¾å·²ä¿å­˜: {comparison_path}")
        plt.show()
    
    def _statistical_comparison(self, exp_results: Dict[str, Any],
                              imp_results: Dict[str, Any],
                              output_dir: str):
        """ç»Ÿè®¡æ¯”è¾ƒ"""
        # æ”¶é›†å‡†ç¡®ç‡æ•°æ®
        exp_accuracies = [exp_results['results'][name]['accuracy'] for name in exp_results['results'].keys()]
        imp_accuracies = [imp_results['results'][name]['accuracy'] for name in imp_results['results'].keys()]
        
        # ç»Ÿè®¡æ£€éªŒ
        t_stat, t_p = ttest_ind(exp_accuracies, imp_accuracies)
        u_stat, u_p = mannwhitneyu(exp_accuracies, imp_accuracies, alternative='two-sided')
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_results = {
            'accuracy_comparison': {
                't_statistic': t_stat,
                't_p_value': t_p,
                'u_statistic': u_stat,
                'u_p_value': u_p,
                'explicit_mean': np.mean(exp_accuracies),
                'implicit_mean': np.mean(imp_accuracies),
                'explicit_std': np.std(exp_accuracies),
                'implicit_std': np.std(imp_accuracies)
            }
        }
        
        stats_path = os.path.join(output_dir, "probe_statistics.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒç»“æœå·²ä¿å­˜: {stats_path}")
        print(f"   å‡†ç¡®ç‡ tæ£€éªŒ: t={t_stat:.4f}, p={t_p:.4f}")
        print(f"   æ˜¾æ€§æ¶æ„å¹³å‡å‡†ç¡®ç‡: {np.mean(exp_accuracies):.4f} Â± {np.std(exp_accuracies):.4f}")
        print(f"   éæ˜¾æ€§æ¶æ„å¹³å‡å‡†ç¡®ç‡: {np.mean(imp_accuracies):.4f} Â± {np.std(imp_accuracies):.4f}")


def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çº¿æ€§æ¢é’ˆè®­ç»ƒ")
    parser.add_argument("--model_dir", type=str, required=True,
                       help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--explicit_data", type=str, required=True,
                       help="æ˜¾æ€§æ¶æ„æ•°æ®æ–‡ä»¶")
    parser.add_argument("--implicit_data", type=str, required=True,
                       help="éæ˜¾æ€§æ¶æ„æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="./probe_analysis",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ç±»å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¢é’ˆè®­ç»ƒå™¨
    trainer = ProbeTrainer(args.model_dir, args.device)
    
    # åŠ è½½æ¨¡å‹
    trainer.load_model()
    
    # åŠ è½½æ•°æ®
    explicit_data = []
    with open(args.explicit_data, 'r', encoding='utf-8') as f:
        for line in f:
            explicit_data.append(json.loads(line.strip()))
    
    implicit_data = []
    with open(args.implicit_data, 'r', encoding='utf-8') as f:
        for line in f:
            implicit_data.append(json.loads(line.strip()))
    
    print(f"ğŸ“Š åŠ è½½æ•°æ®:")
    print(f"   æ˜¾æ€§æ¶æ„æ ·æœ¬: {len(explicit_data)}")
    print(f"   éæ˜¾æ€§æ¶æ„æ ·æœ¬: {len(implicit_data)}")
    
    # æ¯”è¾ƒæ¶æ„
    trainer.compare_architectures(explicit_data, implicit_data, args.output_dir)
    
    # åˆ†æå±‚è¡¨å¾
    all_data = explicit_data + implicit_data
    layer_results = trainer.analyze_layer_representations(all_data, args.output_dir)
    
    print("âœ… æ¢é’ˆè®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
