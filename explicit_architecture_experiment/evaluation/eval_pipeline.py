#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
eval_pipeline.py
----------------------------------------------------
æ˜¾æ€§æ¶æ„å®éªŒè¯„ä¼°ç®¡é“

åŠŸèƒ½ï¼š
1. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
2. å¯¹æµ‹è¯•é›†è¿›è¡Œæ¨ç†
3. è®¡ç®—å¤šç§è¯„ä¼°æŒ‡æ ‡
4. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä¾èµ–ï¼š
    pip install transformers datasets torch evaluate
----------------------------------------------------
"""

import os
import json
import torch
import numpy as np
from typing import Dict, List, Any, Optional
from pathlib import Path
from tqdm import tqdm
import argparse

from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    pipeline
)
from datasets import load_dataset
import evaluate


class CodeCompletionEvaluator:
    """ä»£ç è¡¥å…¨è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        self.metrics = {}
        
        # åŠ è½½è¯„ä¼°æŒ‡æ ‡
        self._load_metrics()
        
    def _load_metrics(self):
        """åŠ è½½è¯„ä¼°æŒ‡æ ‡"""
        try:
            self.bleu_metric = evaluate.load("bleu")
            self.rouge_metric = evaluate.load("rouge")
            self.meteor_metric = evaluate.load("meteor")
        except Exception as e:
            print(f"âš ï¸ éƒ¨åˆ†è¯„ä¼°æŒ‡æ ‡åŠ è½½å¤±è´¥: {e}")
            self.bleu_metric = None
            self.rouge_metric = None
            self.meteor_metric = None
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            use_fast=True
        )
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=self.device
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   è®¾å¤‡: {next(self.model.parameters()).device}")
        print(f"   æ•°æ®ç±»å‹: {next(self.model.parameters()).dtype}")
    
    def generate_completion(self, input_text: str, max_new_tokens: int = 256, 
                          temperature: float = 0.7, do_sample: bool = False) -> str:
        """ç”Ÿæˆä»£ç è¡¥å…¨"""
        # æ£€æŸ¥è¾“å…¥æ–‡æœ¬æ˜¯å¦æœ‰æ•ˆ
        if not input_text or len(input_text.strip()) == 0:
            return ""
        
        # Tokenizeè¾“å…¥
        inputs = self.tokenizer(
            input_text,
            return_tensors="pt",
            truncation=True,
            max_length=1024,
            padding=True
        ).to(self.model.device)
        
        # æ£€æŸ¥tokenizedè¾“å…¥æ˜¯å¦æœ‰æ•ˆ
        if inputs['input_ids'].shape[1] == 0:
            return ""
        
        # Generate
        with torch.no_grad():
            try:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    top_p=0.9,
                    top_k=50,
                    no_repeat_ngram_size=2
                )
            except RuntimeError as e:
                if "probability tensor contains" in str(e):
                    # å¦‚æœæ¦‚ç‡åˆ†å¸ƒæœ‰é—®é¢˜ï¼Œä½¿ç”¨è´ªå©ªè§£ç 
                    print(f"âš ï¸ æ¦‚ç‡åˆ†å¸ƒå¼‚å¸¸ï¼Œä½¿ç”¨è´ªå©ªè§£ç : {e}")
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,  # è´ªå©ªè§£ç 
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        repetition_penalty=1.1
                    )
                else:
                    raise e
        
        # è§£ç è¾“å‡º
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def exact_match(self, prediction: str, reference: str) -> bool:
        """è®¡ç®—ç²¾ç¡®åŒ¹é…"""
        return prediction.strip() == reference.strip()
    
    def code_bleu_score(self, prediction: str, reference: str) -> float:
        """è®¡ç®—CodeBLEUåˆ†æ•°"""
        try:
            # ç®€å•çš„BLEUè®¡ç®—ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„CodeBLEUï¼‰
            if self.bleu_metric is not None:
                result = self.bleu_metric.compute(
                    predictions=[prediction],
                    references=[[reference]]
                )
                return result['bleu']
            else:
                return 0.0
        except Exception:
            return 0.0
    
    def rouge_score(self, prediction: str, reference: str) -> Dict[str, float]:
        """è®¡ç®—ROUGEåˆ†æ•°"""
        try:
            if self.rouge_metric is not None:
                result = self.rouge_metric.compute(
                    predictions=[prediction],
                    references=[reference]
                )
                return {
                    'rouge1': result['rouge1'],
                    'rouge2': result['rouge2'],
                    'rougeL': result['rougeL']
                }
            else:
                return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
        except Exception:
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
    
    def meteor_score(self, prediction: str, reference: str) -> float:
        """è®¡ç®—METEORåˆ†æ•°"""
        try:
            if self.meteor_metric is not None:
                result = self.meteor_metric.compute(
                    predictions=[prediction],
                    references=[reference]
                )
                return result['meteor']
            else:
                return 0.0
        except Exception:
            return 0.0
    
    def evaluate_single_sample(self, sample: Dict[str, Any], 
                              max_new_tokens: int = 256) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        # å…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼
        input_text = sample.get('input', sample.get('prefix', ''))
        reference = sample.get('target', sample.get('suffix', ''))
        
        # è°ƒè¯•ä¿¡æ¯
        if not input_text or len(input_text.strip()) == 0:
            print(f"âš ï¸ è­¦å‘Š: è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æ ·æœ¬")
            return {
                'exact_match': False,
                'bleu_score': 0.0,
                'rouge_scores': {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},
                'meteor_score': 0.0,
                'prediction': '',
                'reference': reference
            }
        
        # ç”Ÿæˆé¢„æµ‹
        prediction = self.generate_completion(input_text, max_new_tokens)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        exact_match = self.exact_match(prediction, reference)
        bleu_score = self.code_bleu_score(prediction, reference)
        rouge_scores = self.rouge_score(prediction, reference)
        meteor_score = self.meteor_score(prediction, reference)
        
        return {
            'input': input_text,
            'reference': reference,
            'prediction': prediction,
            'exact_match': exact_match,
            'bleu': bleu_score,
            'rouge1': rouge_scores['rouge1'],
            'rouge2': rouge_scores['rouge2'],
            'rougeL': rouge_scores['rougeL'],
            'meteor': meteor_score,
            'view': sample.get('view', 'unknown'),
            'coupling': sample.get('coupling', {})
        }
    
    def evaluate_dataset(self, test_file: str, output_file: str = None, 
                        max_new_tokens: int = 256) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä¸ªæ•°æ®é›†"""
        print(f"ğŸ“Š è¯„ä¼°æ•°æ®é›†: {test_file}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        if test_file.endswith('.jsonl'):
            test_data = []
            with open(test_file, 'r', encoding='utf-8') as f:
                for line in f:
                    test_data.append(json.loads(line.strip()))
        elif test_file.endswith('.json'):
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
        
        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        results = []
        for sample in tqdm(test_data, desc="è¯„ä¼°æ ·æœ¬"):
            result = self.evaluate_single_sample(sample, max_new_tokens)
            results.append(result)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._compute_overall_metrics(results)
        
        # æŒ‰æ¶æ„ç±»å‹åˆ†æ
        architecture_analysis = self._analyze_by_architecture(results)
        
        # æŒ‰è€¦åˆåº¦åˆ†æ
        coupling_analysis = self._analyze_by_coupling(results)
        
        # ä¿å­˜ç»“æœ
        if output_file:
            self._save_results(results, overall_metrics, architecture_analysis, 
                              coupling_analysis, output_file)
        
        return {
            'results': results,
            'overall_metrics': overall_metrics,
            'architecture_analysis': architecture_analysis,
            'coupling_analysis': coupling_analysis
        }
    
    def _compute_overall_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—æ€»ä½“æŒ‡æ ‡"""
        if not results:
            return {}
        
        # åŸºæœ¬æŒ‡æ ‡
        exact_matches = [r['exact_match'] for r in results]
        bleu_scores = [r['bleu'] for r in results]
        rouge1_scores = [r['rouge1'] for r in results]
        rouge2_scores = [r['rouge2'] for r in results]
        rougeL_scores = [r['rougeL'] for r in results]
        meteor_scores = [r['meteor'] for r in results]
        
        return {
            'exact_match_rate': np.mean(exact_matches),
            'bleu_mean': np.mean(bleu_scores),
            'bleu_std': np.std(bleu_scores),
            'rouge1_mean': np.mean(rouge1_scores),
            'rouge1_std': np.std(rouge1_scores),
            'rouge2_mean': np.mean(rouge2_scores),
            'rouge2_std': np.std(rouge2_scores),
            'rougeL_mean': np.mean(rougeL_scores),
            'rougeL_std': np.std(rougeL_scores),
            'meteor_mean': np.mean(meteor_scores),
            'meteor_std': np.std(meteor_scores),
            'total_samples': len(results)
        }
    
    def _analyze_by_architecture(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æŒ‰æ¶æ„ç±»å‹åˆ†æç»“æœ"""
        explicit_results = [r for r in results if r.get('view') == 'explicit']
        non_explicit_results = [r for r in results if r.get('view') == 'non_explicit']
        
        analysis = {}
        
        if explicit_results:
            analysis['explicit'] = self._compute_overall_metrics(explicit_results)
        
        if non_explicit_results:
            analysis['non_explicit'] = self._compute_overall_metrics(non_explicit_results)
        
        # æ¯”è¾ƒä¸¤ç§æ¶æ„
        if explicit_results and non_explicit_results:
            exp_em = np.mean([r['exact_match'] for r in explicit_results])
            non_exp_em = np.mean([r['exact_match'] for r in non_explicit_results])
            
            analysis['comparison'] = {
                'exact_match_difference': exp_em - non_exp_em,
                'explicit_samples': len(explicit_results),
                'non_explicit_samples': len(non_explicit_results)
            }
        
        return analysis
    
    def _analyze_by_coupling(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æŒ‰è€¦åˆåº¦åˆ†æç»“æœ"""
        # æŒ‰è€¦åˆåº¦åˆ†ç»„
        low_coupling = []
        medium_coupling = []
        high_coupling = []
        
        for result in results:
            coupling = result.get('coupling', {})
            coupling_score = coupling.get('coupling_score', 0)
            
            if coupling_score < 5:
                low_coupling.append(result)
            elif coupling_score < 15:
                medium_coupling.append(result)
            else:
                high_coupling.append(result)
        
        analysis = {}
        
        if low_coupling:
            analysis['low_coupling'] = self._compute_overall_metrics(low_coupling)
            analysis['low_coupling']['sample_count'] = len(low_coupling)
        
        if medium_coupling:
            analysis['medium_coupling'] = self._compute_overall_metrics(medium_coupling)
            analysis['medium_coupling']['sample_count'] = len(medium_coupling)
        
        if high_coupling:
            analysis['high_coupling'] = self._compute_overall_metrics(high_coupling)
            analysis['high_coupling']['sample_count'] = len(high_coupling)
        
        return analysis
    
    def _save_results(self, results: List[Dict[str, Any]], overall_metrics: Dict[str, float],
                     architecture_analysis: Dict[str, Any], coupling_analysis: Dict[str, Any],
                     output_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_data = {
            'overall_metrics': overall_metrics,
            'architecture_analysis': architecture_analysis,
            'coupling_analysis': coupling_analysis,
            'detailed_results': results
        }
        
        # ä¿å­˜JSONç»“æœ
        json_file = output_file.replace('.jsonl', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: {output_file}")
        print(f"   æ±‡æ€»ç»“æœ: {json_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ˜¾æ€§æ¶æ„å®éªŒè¯„ä¼°")
    parser.add_argument("--model_dir", type=str, required=True,
                       help="å¾®è°ƒåçš„æ¨¡å‹ç›®å½•")
    parser.add_argument("--test_file", type=str, required=True,
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output", type=str, default="./evaluation_results.jsonl",
                       help="è¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--max_new_tokens", type=int, default=256,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ç±»å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = CodeCompletionEvaluator(args.model_dir, args.device)
    
    # åŠ è½½æ¨¡å‹
    evaluator.load_model()
    
    # è¯„ä¼°æ•°æ®é›†
    results = evaluator.evaluate_dataset(
        args.test_file, 
        args.output, 
        args.max_new_tokens
    )
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“ˆ è¯„ä¼°ç»“æœæ‘˜è¦:")
    print(f"   æ€»æ ·æœ¬æ•°: {results['overall_metrics']['total_samples']}")
    print(f"   ç²¾ç¡®åŒ¹é…ç‡: {results['overall_metrics']['exact_match_rate']:.4f}")
    print(f"   BLEUåˆ†æ•°: {results['overall_metrics']['bleu_mean']:.4f}")
    print(f"   ROUGE-Låˆ†æ•°: {results['overall_metrics']['rougeL_mean']:.4f}")
    print(f"   METEORåˆ†æ•°: {results['overall_metrics']['meteor_mean']:.4f}")
    
    # æ¶æ„å¯¹æ¯”
    if 'comparison' in results['architecture_analysis']:
        comp = results['architecture_analysis']['comparison']
        print(f"\nğŸ—ï¸ æ¶æ„å¯¹æ¯”:")
        print(f"   æ˜¾æ€§æ¶æ„æ ·æœ¬: {comp['explicit_samples']}")
        print(f"   éæ˜¾æ€§æ¶æ„æ ·æœ¬: {comp['non_explicit_samples']}")
        print(f"   ç²¾ç¡®åŒ¹é…å·®å¼‚: {comp['exact_match_difference']:.4f}")
    
    print("âœ… è¯„ä¼°å®Œæˆ!")


if __name__ == "__main__":
    main()
