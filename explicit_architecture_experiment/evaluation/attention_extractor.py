#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
attention_extractor.py
----------------------------------------------------
æ³¨æ„åŠ›æœºåˆ¶åˆ†æå·¥å…·

åŠŸèƒ½ï¼š
1. æå–æ¨¡å‹å„å±‚çš„æ³¨æ„åŠ›çŸ©é˜µ
2. åˆ†ææ³¨æ„åŠ›æ¨¡å¼å·®å¼‚
3. è®¡ç®—æ³¨æ„åŠ›ç†µå’Œé›†ä¸­åº¦æŒ‡æ ‡
4. å¯è§†åŒ–æ³¨æ„åŠ›åˆ†å¸ƒ

ä¾èµ–ï¼š
    pip install transformers torch matplotlib seaborn
----------------------------------------------------
"""

import os
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from tqdm import tqdm
import argparse

from transformers import AutoModelForCausalLM, AutoTokenizer


class AttentionAnalyzer:
    """æ³¨æ„åŠ›æœºåˆ¶åˆ†æå™¨"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            use_fast=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¯ç”¨æ³¨æ„åŠ›è¾“å‡ºï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=self.device,
            output_attentions=True
        )
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   å±‚æ•°: {self.model.config.n_layer}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.model.config.n_head}")
    
    def extract_attention(self, text: str, max_length: int = 512) -> Dict[str, np.ndarray]:
        """æå–æ–‡æœ¬çš„æ³¨æ„åŠ›çŸ©é˜µ"""
        # Tokenize
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        ).to(self.model.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.model(**inputs, output_attentions=True)
        
        # æå–æ³¨æ„åŠ›çŸ©é˜µ
        attentions = outputs.attentions  # tuple of (layer, batch, head, seq, seq)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        attention_matrices = {}
        for layer_idx, attention in enumerate(attentions):
            # attention shape: (batch, head, seq, seq)
            attention_np = attention[0].cpu().numpy()  # ç§»é™¤batchç»´åº¦
            attention_matrices[f"layer_{layer_idx}"] = attention_np
        
        return attention_matrices
    
    def compute_attention_entropy(self, attention_matrix: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›ç†µ"""
        # attention_matrix shape: (head, seq, seq)
        # å¯¹æ¯ä¸ªtokenè®¡ç®—å…¶æ³¨æ„åŠ›åˆ†å¸ƒçš„ç†µ
        entropies = []
        for head_idx in range(attention_matrix.shape[0]):
            for token_idx in range(attention_matrix.shape[1]):
                attention_dist = attention_matrix[head_idx, token_idx, :]
                # å½’ä¸€åŒ–
                attention_dist = attention_dist / (attention_dist.sum() + 1e-8)
                # è®¡ç®—ç†µ
                entropy = -np.sum(attention_dist * np.log(attention_dist + 1e-8))
                entropies.append(entropy)
        
        return np.mean(entropies)
    
    def compute_attention_concentration(self, attention_matrix: np.ndarray) -> float:
        """è®¡ç®—æ³¨æ„åŠ›é›†ä¸­åº¦"""
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡çš„é›†ä¸­ç¨‹åº¦
        # ä½¿ç”¨Giniç³»æ•°æˆ–æœ€å¤§æ³¨æ„åŠ›æ¯”ä¾‹
        concentrations = []
        for head_idx in range(attention_matrix.shape[0]):
            for token_idx in range(attention_matrix.shape[1]):
                attention_dist = attention_matrix[head_idx, token_idx, :]
                # å½’ä¸€åŒ–
                attention_dist = attention_dist / (attention_dist.sum() + 1e-8)
                # è®¡ç®—æœ€å¤§æ³¨æ„åŠ›æ¯”ä¾‹
                max_attention = np.max(attention_dist)
                concentrations.append(max_attention)
        
        return np.mean(concentrations)
    
    def compute_module_attention_mass(self, attention_matrix: np.ndarray, 
                                    module_boundaries: List[Tuple[int, int]]) -> float:
        """è®¡ç®—æ¨¡å—å†…æ³¨æ„åŠ›è´¨é‡"""
        if not module_boundaries:
            return 0.0
        
        total_mass = 0.0
        module_mass = 0.0
        
        for head_idx in range(attention_matrix.shape[0]):
            for token_idx in range(attention_matrix.shape[1]):
                attention_dist = attention_matrix[head_idx, token_idx, :]
                total_mass += attention_dist.sum()
                
                # è®¡ç®—æ¨¡å—å†…æ³¨æ„åŠ›
                for start, end in module_boundaries:
                    if start <= token_idx < end:
                        module_mass += attention_dist[start:end].sum()
                        break
        
        return module_mass / (total_mass + 1e-8)
    
    def analyze_attention_patterns(self, attention_matrices: Dict[str, np.ndarray],
                                 module_boundaries: List[Tuple[int, int]] = None) -> Dict[str, Any]:
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
        patterns = {}
        
        for layer_name, attention_matrix in attention_matrices.items():
            # è®¡ç®—å„ç§æŒ‡æ ‡
            entropy = self.compute_attention_entropy(attention_matrix)
            concentration = self.compute_attention_concentration(attention_matrix)
            
            module_mass = 0.0
            if module_boundaries:
                module_mass = self.compute_module_attention_mass(attention_matrix, module_boundaries)
            
            patterns[layer_name] = {
                'entropy': entropy,
                'concentration': concentration,
                'module_mass': module_mass,
                'shape': attention_matrix.shape
            }
        
        return patterns
    
    def visualize_attention(self, attention_matrix: np.ndarray, tokens: List[str],
                          layer_name: str, head_idx: int = 0, 
                          output_path: str = None):
        """å¯è§†åŒ–æ³¨æ„åŠ›çŸ©é˜µ"""
        # é€‰æ‹©ç‰¹å®šå¤´çš„æ³¨æ„åŠ›
        if head_idx >= attention_matrix.shape[0]:
            head_idx = 0
        
        attention_head = attention_matrix[head_idx]
        
        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            attention_head,
            xticklabels=tokens,
            yticklabels=tokens,
            cmap='Blues',
            cbar=True,
            square=True
        )
        
        plt.title(f'Attention Pattern - {layer_name} Head {head_idx}')
        plt.xlabel('Key Tokens')
        plt.ylabel('Query Tokens')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        
        plt.show()
    
    def compare_architectures(self, explicit_attentions: List[Dict[str, np.ndarray]],
                           implicit_attentions: List[Dict[str, np.ndarray]],
                           output_dir: str):
        """æ¯”è¾ƒæ˜¾æ€§å’Œéæ˜¾æ€§æ¶æ„çš„æ³¨æ„åŠ›æ¨¡å¼"""
        ensure_dir(output_dir)
        
        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›æ¨¡å¼
        exp_patterns = self._compute_average_patterns(explicit_attentions)
        imp_patterns = self._compute_average_patterns(implicit_attentions)
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        self._create_comparison_plots(exp_patterns, imp_patterns, output_dir)
        
        # ç»Ÿè®¡æ£€éªŒ
        self._statistical_comparison(exp_patterns, imp_patterns, output_dir)
        
        print(f"ğŸ“Š æ³¨æ„åŠ›å¯¹æ¯”åˆ†æå®Œæˆ: {output_dir}")
    
    def _compute_average_patterns(self, attention_list: List[Dict[str, np.ndarray]]) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—å¹³å‡æ³¨æ„åŠ›æ¨¡å¼"""
        if not attention_list:
            return {}
        
        # æ”¶é›†æ‰€æœ‰å±‚çš„æŒ‡æ ‡
        layer_metrics = {}
        for attention_dict in attention_list:
            for layer_name, attention_matrix in attention_dict.items():
                if layer_name not in layer_metrics:
                    layer_metrics[layer_name] = {
                        'entropies': [],
                        'concentrations': [],
                        'module_masses': []
                    }
                
                patterns = self.analyze_attention_patterns({layer_name: attention_matrix})
                layer_metrics[layer_name]['entropies'].append(patterns[layer_name]['entropy'])
                layer_metrics[layer_name]['concentrations'].append(patterns[layer_name]['concentration'])
                layer_metrics[layer_name]['module_masses'].append(patterns[layer_name]['module_mass'])
        
        # è®¡ç®—å¹³å‡å€¼
        avg_patterns = {}
        for layer_name, metrics in layer_metrics.items():
            avg_patterns[layer_name] = {
                'entropy_mean': np.mean(metrics['entropies']),
                'entropy_std': np.std(metrics['entropies']),
                'concentration_mean': np.mean(metrics['concentrations']),
                'concentration_std': np.std(metrics['concentrations']),
                'module_mass_mean': np.mean(metrics['module_masses']),
                'module_mass_std': np.std(metrics['module_masses'])
            }
        
        return avg_patterns
    
    def _create_comparison_plots(self, exp_patterns: Dict[str, Dict[str, float]],
                                imp_patterns: Dict[str, Dict[str, float]],
                                output_dir: str):
        """åˆ›å»ºå¯¹æ¯”å›¾"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # æå–å±‚æ•°
        layers = list(exp_patterns.keys())
        layer_indices = [int(layer.split('_')[1]) for layer in layers]
        
        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ³¨æ„åŠ›ç†µå¯¹æ¯”
        exp_entropies = [exp_patterns[layer]['entropy_mean'] for layer in layers]
        imp_entropies = [imp_patterns[layer]['entropy_mean'] for layer in layers]
        
        axes[0, 0].plot(layer_indices, exp_entropies, 'o-', label='æ˜¾æ€§æ¶æ„', color='blue')
        axes[0, 0].plot(layer_indices, imp_entropies, 's-', label='éæ˜¾æ€§æ¶æ„', color='red')
        axes[0, 0].set_xlabel('å±‚æ•°')
        axes[0, 0].set_ylabel('å¹³å‡æ³¨æ„åŠ›ç†µ')
        axes[0, 0].set_title('æ³¨æ„åŠ›ç†µå¯¹æ¯”')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ³¨æ„åŠ›é›†ä¸­åº¦å¯¹æ¯”
        exp_concentrations = [exp_patterns[layer]['concentration_mean'] for layer in layers]
        imp_concentrations = [imp_patterns[layer]['concentration_mean'] for layer in layers]
        
        axes[0, 1].plot(layer_indices, exp_concentrations, 'o-', label='æ˜¾æ€§æ¶æ„', color='blue')
        axes[0, 1].plot(layer_indices, imp_concentrations, 's-', label='éæ˜¾æ€§æ¶æ„', color='red')
        axes[0, 1].set_xlabel('å±‚æ•°')
        axes[0, 1].set_ylabel('å¹³å‡æ³¨æ„åŠ›é›†ä¸­åº¦')
        axes[0, 1].set_title('æ³¨æ„åŠ›é›†ä¸­åº¦å¯¹æ¯”')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ¨¡å—å†…æ³¨æ„åŠ›è´¨é‡å¯¹æ¯”
        exp_module_masses = [exp_patterns[layer]['module_mass_mean'] for layer in layers]
        imp_module_masses = [imp_patterns[layer]['module_mass_mean'] for layer in layers]
        
        axes[1, 0].plot(layer_indices, exp_module_masses, 'o-', label='æ˜¾æ€§æ¶æ„', color='blue')
        axes[1, 0].plot(layer_indices, imp_module_masses, 's-', label='éæ˜¾æ€§æ¶æ„', color='red')
        axes[1, 0].set_xlabel('å±‚æ•°')
        axes[1, 0].set_ylabel('å¹³å‡æ¨¡å—å†…æ³¨æ„åŠ›è´¨é‡')
        axes[1, 0].set_title('æ¨¡å—å†…æ³¨æ„åŠ›è´¨é‡å¯¹æ¯”')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ç»¼åˆå¯¹æ¯”
        x = np.arange(len(layers))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, exp_entropies, width, label='æ˜¾æ€§æ¶æ„ç†µ', alpha=0.7)
        axes[1, 1].bar(x + width/2, imp_entropies, width, label='éæ˜¾æ€§æ¶æ„ç†µ', alpha=0.7)
        axes[1, 1].set_xlabel('å±‚æ•°')
        axes[1, 1].set_ylabel('æ³¨æ„åŠ›ç†µ')
        axes[1, 1].set_title('å„å±‚æ³¨æ„åŠ›ç†µå¯¹æ¯”')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(layer_indices)
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        comparison_path = os.path.join(output_dir, "attention_comparison.png")
        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“ˆ æ³¨æ„åŠ›å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_path}")
        plt.show()
    
    def _statistical_comparison(self, exp_patterns: Dict[str, Dict[str, float]],
                              imp_patterns: Dict[str, Dict[str, float]],
                              output_dir: str):
        """ç»Ÿè®¡æ¯”è¾ƒ"""
        from scipy.stats import ttest_ind, mannwhitneyu
        
        # æ”¶é›†æ•°æ®
        exp_entropies = [exp_patterns[layer]['entropy_mean'] for layer in exp_patterns.keys()]
        imp_entropies = [imp_patterns[layer]['entropy_mean'] for layer in imp_patterns.keys()]
        
        exp_concentrations = [exp_patterns[layer]['concentration_mean'] for layer in exp_patterns.keys()]
        imp_concentrations = [imp_patterns[layer]['concentration_mean'] for layer in imp_patterns.keys()]
        
        # ç»Ÿè®¡æ£€éªŒ
        t_stat_entropy, t_p_entropy = ttest_ind(exp_entropies, imp_entropies)
        u_stat_entropy, u_p_entropy = mannwhitneyu(exp_entropies, imp_entropies, alternative='two-sided')
        
        t_stat_conc, t_p_conc = ttest_ind(exp_concentrations, imp_concentrations)
        u_stat_conc, u_p_conc = mannwhitneyu(exp_concentrations, imp_concentrations, alternative='two-sided')
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_results = {
            'entropy_comparison': {
                't_statistic': t_stat_entropy,
                't_p_value': t_p_entropy,
                'u_statistic': u_stat_entropy,
                'u_p_value': u_p_entropy,
                'explicit_mean': np.mean(exp_entropies),
                'implicit_mean': np.mean(imp_entropies)
            },
            'concentration_comparison': {
                't_statistic': t_stat_conc,
                't_p_value': t_p_conc,
                'u_statistic': u_stat_conc,
                'u_p_value': u_p_conc,
                'explicit_mean': np.mean(exp_concentrations),
                'implicit_mean': np.mean(imp_concentrations)
            }
        }
        
        stats_path = os.path.join(output_dir, "attention_statistics.json")
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒç»“æœå·²ä¿å­˜: {stats_path}")
        print(f"   æ³¨æ„åŠ›ç†µ tæ£€éªŒ: t={t_stat_entropy:.4f}, p={t_p_entropy:.4f}")
        print(f"   æ³¨æ„åŠ›é›†ä¸­åº¦ tæ£€éªŒ: t={t_stat_conc:.4f}, p={t_p_conc:.4f}")


def ensure_dir(path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ³¨æ„åŠ›æœºåˆ¶åˆ†æ")
    parser.add_argument("--model_dir", type=str, required=True,
                       help="æ¨¡å‹ç›®å½•")
    parser.add_argument("--test_file", type=str, required=True,
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="./attention_analysis",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_length", type=int, default=512,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ç±»å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AttentionAnalyzer(args.model_dir, args.device)
    
    # åŠ è½½æ¨¡å‹
    analyzer.load_model()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = []
    with open(args.test_file, 'r', encoding='utf-8') as f:
        for line in f:
            test_data.append(json.loads(line.strip()))
    
    print(f"ğŸ“Š åˆ†æ {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # æå–æ³¨æ„åŠ›
    explicit_attentions = []
    implicit_attentions = []
    
    for sample in tqdm(test_data, desc="æå–æ³¨æ„åŠ›"):
        text = sample.get('input', '')
        view = sample.get('view', '')
        
        try:
            attention_matrices = analyzer.extract_attention(text, args.max_length)
            
            if view == 'explicit':
                explicit_attentions.append(attention_matrices)
            elif view == 'non_explicit':
                implicit_attentions.append(attention_matrices)
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
            continue
    
    print(f"   æ˜¾æ€§æ¶æ„æ ·æœ¬: {len(explicit_attentions)}")
    print(f"   éæ˜¾æ€§æ¶æ„æ ·æœ¬: {len(implicit_attentions)}")
    
    # æ¯”è¾ƒåˆ†æ
    if explicit_attentions and implicit_attentions:
        analyzer.compare_architectures(explicit_attentions, implicit_attentions, args.output_dir)
    
    print("âœ… æ³¨æ„åŠ›åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()
