#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
finetune.py
----------------------------------------------------
æ˜¾æ€§æ¶æ„å®éªŒæ¨¡å‹å¾®è°ƒè„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer
2. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæ˜¾æ€§/éæ˜¾æ€§æ¶æ„æ ·æœ¬ï¼‰
3. å¾®è°ƒæ¨¡å‹ç”¨äºå‡½æ•°è¡¥å…¨ä»»åŠ¡
4. æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥

ä¾èµ–ï¼š
    pip install transformers datasets torch accelerate
----------------------------------------------------
"""

import os
import json
import math
import random
import numpy as np
from dataclasses import dataclass, field
from typing import Optional, Dict, List, Any
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoConfig, AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling,
    set_seed, EarlyStoppingCallback
)
from datasets import load_dataset, Dataset as HFDataset
from tqdm import tqdm


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_name_or_path: str = "gpt2"
    output_dir: str = "./outputs/finetune"
    train_file: str = "./data/tasks/train.jsonl"
    validation_file: str = "./data/tasks/val.jsonl"
    test_file: str = "./data/tasks/test.jsonl"
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 5e-5
    weight_decay: float = 0.01
    num_train_epochs: int = 3
    seed: int = 42
    max_length: int = 1024
    mask_token: str = "[MASKED_FUNCTION_BODY]"
    padding_side: str = "right"
    warmup_steps: int = 100
    logging_steps: int = 50
    save_steps: int = 500
    eval_steps: int = 500
    early_stopping_patience: int = 3
    fp16: bool = True
    dataloader_num_workers: int = 4


class CodeCompletionDataset(Dataset):
    """ä»£ç è¡¥å…¨æ•°æ®é›†"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 1024, mask_token: str = "[MASKED_FUNCTION_BODY]"):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mask_token = mask_token
        
        # åŠ è½½æ•°æ®
        self.data = self._load_data(data_path)
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {len(self.data)} ä¸ªæ ·æœ¬")
        
    def _load_data(self, data_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        data = []
        if data_path.endswith('.jsonl'):
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    data.append(json.loads(line.strip()))
        elif data_path.endswith('.json'):
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # è·å–è¾“å…¥å’Œç›®æ ‡ï¼ˆå…¼å®¹ä¸åŒæ•°æ®æ ¼å¼ï¼‰
        input_text = item.get('input', item.get('prefix', ''))
        target_text = item.get('target', item.get('suffix', ''))
        
        # éªŒè¯æ•°æ®
        if not input_text or not target_text:
            print(f"âš ï¸ è­¦å‘Š: æ ·æœ¬ {idx} æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„æœ‰æ•ˆæ ·æœ¬
            input_text = "def hello():"
            target_text = "    print('Hello World')"
        
        # åˆ›å»ºå®Œæ•´çš„åºåˆ—ï¼šè¾“å…¥ + ç›®æ ‡
        full_text = input_text + target_text
        
        # Tokenizeå®Œæ•´åºåˆ—ï¼ˆå›ºå®šé•¿åº¦ï¼Œä¾¿äºæ‰¹å¤„ç†å¯¹é½ï¼‰
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors="pt"
        )
        
        # è®¡ç®—è¾“å…¥éƒ¨åˆ†çš„é•¿åº¦ï¼ˆåŒæ ·å›ºå®šé•¿åº¦ç¼–ç ï¼Œä»…ç”¨äºè·å¾—è¾“å…¥é•¿åº¦ï¼‰
        input_encoding = self.tokenizer(
            input_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors="pt"
        )
        
        input_length = len(input_encoding['input_ids'][0])
        total_length = len(encoding['input_ids'][0])
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç›®æ ‡token
        if input_length >= total_length:
            input_length = max(1, total_length - 1)
        
        # åˆ›å»ºæ ‡ç­¾ï¼šè¾“å…¥éƒ¨åˆ†ä¸º-100ï¼Œç›®æ ‡éƒ¨åˆ†ä¸ºå®é™…token ids
        labels = [-100] * input_length + encoding['input_ids'][0][input_length:].tolist()
        
        # ç¡®ä¿labelså’Œinput_idsé•¿åº¦ä¸€è‡´
        if len(labels) != total_length:
            if len(labels) < total_length:
                labels.extend([-100] * (total_length - len(labels)))
            else:
                labels = labels[:total_length]
        
        # éªŒè¯æ ‡ç­¾æœ‰æ•ˆæ€§ï¼ˆæç«¯æƒ…å†µä¸‹å†æ¬¡ä¿è¯è‡³å°‘1ä¸ªæœ‰æ•ˆæ ‡ç­¾ï¼‰
        if all(l == -100 for l in labels):
            labels[-1] = int(encoding['input_ids'][0][-1].item())
        
        return {
            'input_ids': encoding['input_ids'][0],
            'attention_mask': encoding['attention_mask'][0],
            'labels': torch.tensor(labels, dtype=torch.long)
        }


class CodeCompletionTrainer:
    """ä»£ç è¡¥å…¨è®­ç»ƒå™¨"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.trainer = None
        
        # è®¾ç½®éšæœºç§å­
        set_seed(config.seed)
        
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {self.config.model_name_or_path}")
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name_or_path,
            use_fast=True
        )
        
        # æ·»åŠ ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
        
        # è®¾ç½®paddingæ–¹å‘
        self.tokenizer.padding_side = self.config.padding_side
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name_or_path,
            torch_dtype=torch.float16 if self.config.fp16 else torch.float32
        )
        
        # è°ƒæ•´token embeddingå¤§å°
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer)}")
        print(f"   æ¨¡å‹å‚æ•°: {self.model.num_parameters():,}")
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        train_dataset = CodeCompletionDataset(
            self.config.train_file,
            self.tokenizer,
            self.config.max_length,
            self.config.mask_token
        )
        
        val_dataset = CodeCompletionDataset(
            self.config.validation_file,
            self.tokenizer,
            self.config.max_length,
            self.config.mask_token
        )
        
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªæ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªæ ·æœ¬")
        
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬
        if len(train_dataset) > 0:
            sample = train_dataset[0]
            print(f"ğŸ” æ ·æœ¬è°ƒè¯•:")
            print(f"   input_ids shape: {sample['input_ids'].shape}")
            print(f"   labels shape: {sample['labels'].shape}")
            print(f"   labelsä¸­æœ‰-100çš„æ•°é‡: {(sample['labels'] == -100).sum().item()}")
            print(f"   labelsä¸­æœ‰æ•ˆtokenæ•°é‡: {(sample['labels'] != -100).sum().item()}")
        
        return train_dataset, val_dataset
    
    def setup_trainer(self, train_dataset, val_dataset):
        """è®¾ç½®è®­ç»ƒå™¨"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒå™¨...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_train_epochs=self.config.num_train_epochs,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            warmup_steps=self.config.warmup_steps,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=self.config.fp16,
            dataloader_num_workers=self.config.dataloader_num_workers,
            seed=self.config.seed,
            report_to="none",  # ç¦ç”¨wandbç­‰
            remove_unused_columns=False,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡
            pad_to_multiple_of=8 if self.config.fp16 else None
        )
        
        # æ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=self.config.early_stopping_patience
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            callbacks=[early_stopping]
        )
        
        print("âœ… è®­ç»ƒå™¨è®¾ç½®å®Œæˆ")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # è®­ç»ƒå‰è¯„ä¼°
        print("ğŸ“Š è®­ç»ƒå‰è¯„ä¼°...")
        eval_results = self.trainer.evaluate()
        eval_loss = eval_results.get('eval_loss', float('nan'))
        if not (isinstance(eval_loss, (int, float)) and not (eval_loss != eval_loss)):  # æ£€æŸ¥æ˜¯å¦ä¸ºnan
            print(f"   åˆå§‹éªŒè¯æŸå¤±: {eval_loss:.4f}")
        else:
            print(f"   åˆå§‹éªŒè¯æŸå¤±: nan (æ•°å€¼ä¸ç¨³å®š)")
        
        # å¼€å§‹è®­ç»ƒ
        train_results = self.trainer.train()
        
        # è®­ç»ƒåè¯„ä¼°
        print("ğŸ“Š è®­ç»ƒåè¯„ä¼°...")
        final_eval_results = self.trainer.evaluate()
        eval_loss = final_eval_results.get('eval_loss', float('nan'))
        if not (isinstance(eval_loss, (int, float)) and not (eval_loss != eval_loss)):  # æ£€æŸ¥æ˜¯å¦ä¸ºnan
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: {eval_loss:.4f}")
        else:
            print(f"   æœ€ç»ˆéªŒè¯æŸå¤±: nan (æ•°å€¼ä¸ç¨³å®š)")
        
        # ä¿å­˜æ¨¡å‹
        self.trainer.save_model()
        self.tokenizer.save_pretrained(self.config.output_dir)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   æ¨¡å‹å·²ä¿å­˜è‡³: {self.config.output_dir}")
        
        return train_results, final_eval_results
    
    def evaluate_model(self, test_file: str = None):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if test_file is None:
            test_file = self.config.test_file
        
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½: {test_file}")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_dataset = CodeCompletionDataset(
            test_file,
            self.tokenizer,
            self.config.max_length,
            self.config.mask_token
        )
        
        # è¯„ä¼°
        eval_results = self.trainer.evaluate(test_dataset)
        
        print("ğŸ“ˆ æµ‹è¯•ç»“æœ:")
        for key, value in eval_results.items():
            print(f"   {key}: {value:.4f}")
        
        return eval_results


def load_config(config_path: str) -> ModelConfig:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    return ModelConfig(**config_dict)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ˜¾æ€§æ¶æ„å®éªŒæ¨¡å‹å¾®è°ƒ")
    parser.add_argument("--config", type=str, default="configs/finetune_config.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model_name", type=str, default="gpt2",
                       help="é¢„è®­ç»ƒæ¨¡å‹åç§°")
    parser.add_argument("--output_dir", type=str, default="./outputs/finetune",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--train_file", type=str, required=True,
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶")
    parser.add_argument("--val_file", type=str, required=True,
                       help="éªŒè¯æ•°æ®æ–‡ä»¶")
    parser.add_argument("--test_file", type=str, default=None,
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶")
    parser.add_argument("--epochs", type=int, default=3,
                       help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                       help="å­¦ä¹ ç‡")
    parser.add_argument("--max_length", type=int, default=1024,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = ModelConfig(
        model_name_or_path=args.model_name,
        output_dir=args.output_dir,
        train_file=args.train_file,
        validation_file=args.val_file,
        test_file=args.test_file,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        max_length=args.max_length,
        seed=args.seed
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CodeCompletionTrainer(config)
    
    # è®¾ç½®æ¨¡å‹
    trainer.setup_model()
    
    # å‡†å¤‡æ•°æ®
    train_dataset, val_dataset = trainer.prepare_data()
    
    # è®¾ç½®è®­ç»ƒå™¨
    trainer.setup_trainer(train_dataset, val_dataset)
    
    # å¼€å§‹è®­ç»ƒ
    train_results, eval_results = trainer.train()
    
    # è¯„ä¼°æ¨¡å‹
    if args.test_file:
        test_results = trainer.evaluate_model(args.test_file)
    
    print("ğŸ¯ è®­ç»ƒæµç¨‹å®Œæˆ!")


if __name__ == "__main__":
    main()
